{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyalgur1/AutoEncoders/blob/main/AutoEncoder_CNN_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o_f1PQvuAC6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.optim import Optimizer\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import pdb  # for debugging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pXHj1vD2bwUZ"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for CIFAR10 training set (set by the user)\n",
        "# Note: CIFAR10 training set contains 50000 images\n",
        "\n",
        "batch_size = 125  # batch_size=125 is divisable by 50000 (division is not necessary, but better for plotting)\n",
        "shuffle = True  # shuffle the training set\n",
        "seed_base = random.randint(0, 100)  # set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ga5GDXs0cENm"
      },
      "outputs": [],
      "source": [
        "# Hyperparmeters for training of models (set by the user)\n",
        "# Joint Model: a standard ADAM optimizer model that updates all weights in every iteration.\n",
        "# NAM Model: updates the encoder and the decoder in an alternating fashion using ADAM.\n",
        "\n",
        "\n",
        "# Note 1:\n",
        "# In the current implementation of NAM, the update of the encoder and decoder switches at the end of each epoch.\n",
        "# This means that after completing an epoch, the next epoch begins with the update of the other one (hence, in a single epoch only one of them is updated).\n",
        "# This switching strategy can be modified; for instance, they can switch roles within each mini-batch.\n",
        "# In this alternative approach, the decoder would be trained on a mini-batch, and then the encoder update would switch for the same mini-batch.\n",
        "\n",
        "\n",
        "# Note 2:\n",
        "# The implementation of all models below allows to set a maximal number of iterations for the epochs.\n",
        "# Recall that one iteraion corresponds to a single batch, hence the maximal possible number of iterations at each epoch is len(trainloader.dataset)//batch_size.\n",
        "# This means that by limiting the number ot iterations for an epochs, implies that not all batches will be considered for this epoch.\n",
        "\n",
        "\n",
        "num_epochs = 8\n",
        "num_trials = 20  # number of trials for averaging the loss (over different initializations)\n",
        "joint_model = True  # Set True for Joint Model training\n",
        "NAM_model = True  # Set True for NAM Model training\n",
        "decoder_start = True  # if decoder_start=True, NAM starts with decoder updates, else encoder updates (required for if NAM_model=True); See Note 1 above\n",
        "lr_joint = 0.001  # lr for Joint Model (set 0.001 for ADAM's default)\n",
        "lr_decoder, lr_encoder = (0.01, 0.0001)  # lr for NAM Model's decoder/encoder update (set 0.001 for ADAM's default)\n",
        "num_iter_joint = 'max'  # 'max' or inetger lower than 'max' (otherwise it is set automatically to 'max'); limiting the number of iterations of the epochs of the Joint Model; See Note 2 above\n",
        "num_iter_decoder, num_iter_encoder = ('max', 'max')  # 'max' or inetger lower than 'max' (otherwise it set automatically to 'max'); limiting the number of iterations of the epochs of the NAM Model; See Note 2 above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vqBP_RKiADr4"
      },
      "outputs": [],
      "source": [
        "# Define an AutoEncoder for 3×32×32 CIFAR10 photos\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 12, 4, stride=2, padding=1),  # Takes input shape [batch, channel=3, H=32, W=32] and transforms it to [batch, 12, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
        "            nn.ReLU(),\n",
        "\t\t\tnn.Conv2d(24, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # Takes input shape [batch, channel=48, H=4, W=4] and transforms it to [batch, 12, 16, 16]\n",
        "            nn.ReLU(),\n",
        "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),  # [batch, 3, 32, 32]\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        decoded = decoded.view(x.size(0), 3, 32, 32)  # reshape to original image size\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KVb8NzzGOhJy"
      },
      "outputs": [],
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "def set_seed(seed):  # set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def switch_requires_grad(model, decoder):  # switch requires_grad for model object containing encoder and decoder\n",
        "    if decoder:\n",
        "        for param in model.decoder.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        for param in model.encoder.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.decoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "def reset_learning_rate(optimizer, lr):  # reset learning rate for optimizer object\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "# This function should be redefined to lr annealing.\n",
        "# This function is currently unused below.\n",
        "\n",
        "\n",
        "def NAM_epoch_iterations(decoder_update):  # sets the number of iterations for NAM Model epoch\n",
        "    if decoder_update:  # this is a decoder epoch, so set decoder number of iterations\n",
        "        num_iter_NAM = num_iter_decoder\n",
        "    else:  # this is an encoder epoch, so set encoder number of iterations\n",
        "        num_iter_NAM = num_iter_encoder\n",
        "    return num_iter_NAM\n",
        "\n",
        "\n",
        "def check_num_iter(num_iter_joint, num_iter_decoder, num_iter_encoder, num_iter):  # checks and sets the number of iterations for the epochs\n",
        "    if (num_iter_joint == 'max') or (num_iter_joint > num_iter):\n",
        "        num_iter_joint = num_iter\n",
        "    if (num_iter_decoder == 'max') or (num_iter_decoder > num_iter):\n",
        "        num_iter_decoder = num_iter\n",
        "    if (num_iter_encoder == 'max') or (num_iter_encoder > num_iter):\n",
        "        num_iter_encoder = num_iter\n",
        "    return num_iter_joint, num_iter_decoder, num_iter_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ75u5gACgaC",
        "outputId": "20d5ba88-95a0-4825-9672-93e76d729cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Set training data (CIFAR10)\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "num_iter = int(len(trainloader.dataset)//batch_size)  # maximum number of iterations per epoch for plotting purposes as it is set automatically by PyTorch)\n",
        "num_iter_joint, num_iter_decoder, num_iter_encoder = check_num_iter(num_iter_joint, num_iter_decoder, num_iter_encoder, num_iter)  # set the number of iterations for the epochs\n",
        "\n",
        "\n",
        "# Set loss suitable for autoencoders\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9gpC7c-AALtZ"
      },
      "outputs": [],
      "source": [
        "# Training loop for Joint Model\n",
        "\n",
        "def joint_model_training(seed):\n",
        "    set_seed(seed)  # set seed for reproducibility\n",
        "    model = Autoencoder()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_joint)  # optimizer for the Joint Model\n",
        "    loss_val, num_updates, timing, iteration = ({}, {}, {}, 0)  # loss, number of total weight updates per epoch, time of each epoch, total iteration counter (across epochs) for plotting purposes\n",
        "    total_params = sum(p.numel() for p in model.parameters())  # total number of parameters for the Joint Model\n",
        "\n",
        "    print(f'\\n     Joint Model Training:')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss, epoch_iter, epoch_start = ([], [], time.time())  # loss of epoch iterations, epoch total iteraion counter, start time of each epoch\n",
        "        for i, batch in enumerate(trainloader):  # for each batch and its index\n",
        "            if i == num_iter_joint:  # limit the number of iterations of the epochs\n",
        "                break\n",
        "            inputs, _ = batch  # get the inputs (labels are not required for autoencoders)\n",
        "\n",
        "            optimizer.zero_grad()  # clear the gradients of all autoencoder parameters\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, inputs)\n",
        "            loss.backward()  # calculate partial_L/partial_w for all parameters\n",
        "            optimizer.step()  # update step for all parameters\n",
        "            epoch_loss.append(loss.item())\n",
        "            epoch_iter.append(iteration)\n",
        "            iteration += 1\n",
        "\n",
        "        # Epoch data collection\n",
        "        timing[epoch+1] = time.time() - epoch_start  # time of each epoch\n",
        "        print(f'        Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Run time: {timing[epoch+1]:.2f} seconds')\n",
        "        num_updates[epoch+1] = i * total_params # total weight updates in the epoch (in this model all weights are updates in every batch)\n",
        "        loss_val[epoch+1] = (epoch_iter, epoch_loss)  # loss in the epoch\n",
        "\n",
        "    return model, loss_val, num_updates, timing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ohl9yuc4pz-o"
      },
      "outputs": [],
      "source": [
        "# Training loop for NAM Model\n",
        "\n",
        "def NAM_model_training(seed):\n",
        "    set_seed(seed)  # set seed for reproducibility\n",
        "    model = Autoencoder()\n",
        "    optimizer_decoder = torch.optim.Adam(model.decoder.parameters(), lr=lr_decoder)  # decoder optimizer for NAM\n",
        "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=lr_encoder)  # encoder optimizer for NAM\n",
        "    loss_val, num_updates, timing, iteration = ({}, {}, {}, 0)  # loss, number of total weight updates per epoch, time of each epoch, total iteration counter (across epochs) for plotting purposes\n",
        "    total_decoder_params = sum(p.numel() for p in model.decoder.parameters())  # total number of decoder parameters\n",
        "    total_encoder_params = sum(p.numel() for p in model.encoder.parameters())  # total number of encoder parameters\n",
        "\n",
        "    decoder_update = decoder_start  # switch between encoder and decoder (if decoder_update=Ture, then update decoder, else update encoder)\n",
        "    num_iter_NAM = NAM_epoch_iterations(decoder_update)  # sets the number of iterations for the first epoch, according to whether the epoch corresponds to decoder/encoder\n",
        "\n",
        "    # setting requires_grad=False for the non-updated parts could potentially save some computation by avoiding the calculation of gradients for those parameters.\n",
        "    switch_requires_grad(model, decoder_update)  # switch requires_grad on and off according to the first epoch update (decoder and encoder)\n",
        "\n",
        "    print(f'\\n     NAM Model Training:')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss, epoch_iter, epoch_start = ([], [], time.time())  # loss of epoch iterations, epoch total iteraion counter, start time of each epoch\n",
        "\n",
        "        for i, data in enumerate(trainloader):  # for each batch and its index\n",
        "            if i == num_iter_NAM:  # limit the number of iterations of the epochs\n",
        "                break\n",
        "            inputs, _ = data  # get the inputs (labels are not required for autoencoders)\n",
        "\n",
        "            if decoder_update:\n",
        "                optimizer_decoder.zero_grad()  # clear the gradients of all decoder parameters\n",
        "            else:\n",
        "                optimizer_encoder.zero_grad()  # clear the gradients of all encoder parameters\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, inputs)\n",
        "            loss.backward()  # calculate partial_L/partial_x for all parameters\n",
        "\n",
        "            if decoder_update:\n",
        "                optimizer_decoder.step()  # update step for decoder parameters\n",
        "            else:\n",
        "                optimizer_encoder.step()  # update step for encoder parameters\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            epoch_iter.append(iteration)\n",
        "            iteration += 1\n",
        "\n",
        "        # Epoch data collection\n",
        "        time_end = time.time() - epoch_start\n",
        "        if decoder_update:\n",
        "            timing[epoch+1] = (time_end, 'd')  # time of each epoch\n",
        "            print(f'        Epoch [{epoch + 1}/{num_epochs}] (decoder), Loss: {loss.item():.4f}, Run time: {time_end:.2f} seconds')\n",
        "            loss_val[epoch+1] = (epoch_iter, epoch_loss, 'd')\n",
        "            num_updates[epoch+1] = (i * total_decoder_params, 'd')  # total weight updates in the epoch\n",
        "        else:\n",
        "            timing[epoch+1] = (time_end, 'e')  # time of each epoch\n",
        "            print(f'        Epoch [{epoch + 1}/{num_epochs}] (encoder), Loss: {loss.item():.4f}, Run time: {time_end:.2f} seconds')\n",
        "            loss_val[epoch+1] = (epoch_iter, epoch_loss, 'e')\n",
        "            num_updates[epoch+1] = (i * total_encoder_params, 'e')  # total weight updates in the epoch\n",
        "\n",
        "        decoder_update = not(decoder_update)  # switch between encoder and decoder\n",
        "        switch_requires_grad(model, decoder_update)  # switch requires_grad on and off according to the next epoch update (decoder and encoder)\n",
        "        num_iter_NAM = NAM_epoch_iterations(decoder_update)  # sets the number of iterations for the next epoch, according to whether the epoch corresponds to decoder/encoder\n",
        "\n",
        "    return model, loss_val, num_updates, timing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "riINW5BHT20w"
      },
      "outputs": [],
      "source": [
        "def average_loss_trials(NAM_model, all_losses):\n",
        "    if NAM_model:\n",
        "        avg_loss = {epoch+1: ([], [], all_losses[0][epoch+1][2]) for epoch in range(num_epochs)}  # Initialize avg_loss_NAM based on the encoder/decoder structure of the first trial\n",
        "    else:\n",
        "        avg_loss = {epoch+1: ([], []) for epoch in range(num_epochs)}  # Initialize avg_loss_joint\n",
        "    # Average losses per iteration for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        total_losses = np.zeros_like(all_losses[0][epoch+1][1])\n",
        "        for trial in range(num_trials):\n",
        "            total_losses += np.array(all_losses[trial][epoch+1][1])\n",
        "        if NAM_model:\n",
        "            avg_loss[epoch+1] = (all_losses[0][epoch+1][0], total_losses / num_trials, all_losses[0][epoch+1][2])\n",
        "        else:\n",
        "            avg_loss[epoch+1] = (all_losses[0][epoch+1][0], total_losses / num_trials)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def average_time_trials(NAM_model, all_time):\n",
        "    if NAM_model:\n",
        "        avg_time = {epoch+1: ([], all_time[0][epoch+1][1]) for epoch in range(num_epochs)}  # Initialize avg_time_NAM based on\n",
        "    else:\n",
        "        avg_time = {epoch+1: [] for epoch in range(num_epochs)}  # Initialize avg_time_joint\n",
        "    # Average time per iteration for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "        if NAM_model:\n",
        "            total_time = np.zeros_like(all_time[0][epoch+1][0])\n",
        "            for trial in range(num_trials):\n",
        "                total_time += np.array(all_time[trial][epoch+1][0])\n",
        "            avg_time[epoch+1] = (total_time / num_trials, all_time[0][epoch+1][1])\n",
        "        else:\n",
        "            total_time = np.zeros_like(all_time[0][epoch+1])\n",
        "            for trial in range(num_trials):\n",
        "                total_time += np.array(all_time[trial][epoch+1])\n",
        "            avg_time[epoch+1] = total_time / num_trials\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "def average_training_trials(seed_base):\n",
        "    all_joint_losses, all_NAM_losses, all_joint_time, all_NAM_time = ([], [], [], [])\n",
        "\n",
        "    # Run multiple trials\n",
        "    for trial in range(num_trials):\n",
        "        print(f'\\n Trial {trial + 1}/{num_trials}')\n",
        "        seed = seed_base + trial\n",
        "\n",
        "        # Joint Model Training\n",
        "        if joint_model:\n",
        "            _, loss_joint, param_count_joint, time_joint = joint_model_training(seed)\n",
        "            all_joint_losses.append(loss_joint)\n",
        "            all_joint_time.append(time_joint)\n",
        "\n",
        "        # NAM Model Training\n",
        "        if NAM_model:\n",
        "            _, loss_NAM, param_count_NAM, time_NAM = NAM_model_training(seed)\n",
        "            all_NAM_losses.append(loss_NAM)\n",
        "            all_NAM_time.append(time_NAM)\n",
        "\n",
        "    # Average losses\n",
        "    if joint_model:\n",
        "        avg_loss_joint = average_loss_trials(0, all_joint_losses)\n",
        "        avg_time_joint = average_time_trials(0, all_joint_time)\n",
        "    if NAM_model:\n",
        "        avg_loss_NAM = average_loss_trials(1, all_NAM_losses)\n",
        "        avg_time_NAM = average_time_trials(1, all_NAM_time)\n",
        "\n",
        "    return avg_loss_joint, avg_loss_NAM, param_count_joint, param_count_NAM, avg_time_joint, avg_time_NAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKSixGMfqUwW",
        "outputId": "43ce0687-b4eb-4417-82c2-d22c1a20c811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Trial 1/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0111, Run time: 45.81 seconds\n",
            "        Epoch [2/8], Loss: 0.0095, Run time: 32.23 seconds\n",
            "        Epoch [3/8], Loss: 0.0081, Run time: 34.37 seconds\n",
            "        Epoch [4/8], Loss: 0.0067, Run time: 35.43 seconds\n",
            "        Epoch [5/8], Loss: 0.0062, Run time: 33.05 seconds\n",
            "        Epoch [6/8], Loss: 0.0057, Run time: 33.61 seconds\n",
            "        Epoch [7/8], Loss: 0.0052, Run time: 32.26 seconds\n",
            "        Epoch [8/8], Loss: 0.0052, Run time: 33.75 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0105, Run time: 26.79 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0089, Run time: 28.98 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0079, Run time: 26.56 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0067, Run time: 29.78 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0064, Run time: 26.69 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0056, Run time: 29.26 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0054, Run time: 26.27 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0053, Run time: 28.76 seconds\n",
            "\n",
            " Trial 2/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0121, Run time: 35.38 seconds\n",
            "        Epoch [2/8], Loss: 0.0083, Run time: 31.72 seconds\n",
            "        Epoch [3/8], Loss: 0.0068, Run time: 33.04 seconds\n",
            "        Epoch [4/8], Loss: 0.0063, Run time: 32.26 seconds\n",
            "        Epoch [5/8], Loss: 0.0060, Run time: 33.23 seconds\n",
            "        Epoch [6/8], Loss: 0.0054, Run time: 32.06 seconds\n",
            "        Epoch [7/8], Loss: 0.0050, Run time: 33.30 seconds\n",
            "        Epoch [8/8], Loss: 0.0066, Run time: 32.37 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0175, Run time: 26.62 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0124, Run time: 29.60 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0098, Run time: 26.29 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0085, Run time: 28.86 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0082, Run time: 26.74 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0068, Run time: 28.67 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0064, Run time: 26.06 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0068, Run time: 29.40 seconds\n",
            "\n",
            " Trial 3/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0147, Run time: 31.90 seconds\n",
            "        Epoch [2/8], Loss: 0.0098, Run time: 33.10 seconds\n",
            "        Epoch [3/8], Loss: 0.0083, Run time: 31.84 seconds\n",
            "        Epoch [4/8], Loss: 0.0077, Run time: 32.94 seconds\n",
            "        Epoch [5/8], Loss: 0.0059, Run time: 32.92 seconds\n",
            "        Epoch [6/8], Loss: 0.0060, Run time: 34.80 seconds\n",
            "        Epoch [7/8], Loss: 0.0053, Run time: 32.94 seconds\n",
            "        Epoch [8/8], Loss: 0.0049, Run time: 32.30 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0155, Run time: 26.45 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0107, Run time: 28.70 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0096, Run time: 26.21 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0082, Run time: 29.62 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0068, Run time: 26.16 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0065, Run time: 28.72 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0063, Run time: 26.15 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0056, Run time: 28.42 seconds\n",
            "\n",
            " Trial 4/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0123, Run time: 32.92 seconds\n",
            "        Epoch [2/8], Loss: 0.0093, Run time: 32.16 seconds\n",
            "        Epoch [3/8], Loss: 0.0073, Run time: 33.63 seconds\n",
            "        Epoch [4/8], Loss: 0.0069, Run time: 32.18 seconds\n",
            "        Epoch [5/8], Loss: 0.0058, Run time: 32.91 seconds\n",
            "        Epoch [6/8], Loss: 0.0056, Run time: 31.89 seconds\n",
            "        Epoch [7/8], Loss: 0.0055, Run time: 33.18 seconds\n",
            "        Epoch [8/8], Loss: 0.0047, Run time: 31.93 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0151, Run time: 26.09 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0122, Run time: 30.51 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0085, Run time: 27.55 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0079, Run time: 29.65 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0072, Run time: 27.71 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0069, Run time: 31.09 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0070, Run time: 27.25 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0058, Run time: 28.89 seconds\n",
            "\n",
            " Trial 5/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0121, Run time: 32.29 seconds\n",
            "        Epoch [2/8], Loss: 0.0080, Run time: 32.35 seconds\n",
            "        Epoch [3/8], Loss: 0.0069, Run time: 32.11 seconds\n",
            "        Epoch [4/8], Loss: 0.0065, Run time: 32.43 seconds\n",
            "        Epoch [5/8], Loss: 0.0057, Run time: 32.66 seconds\n",
            "        Epoch [6/8], Loss: 0.0051, Run time: 32.19 seconds\n",
            "        Epoch [7/8], Loss: 0.0045, Run time: 32.68 seconds\n",
            "        Epoch [8/8], Loss: 0.0046, Run time: 33.25 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0128, Run time: 27.70 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0086, Run time: 29.21 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0077, Run time: 26.67 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0071, Run time: 29.01 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0065, Run time: 26.58 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0055, Run time: 29.28 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0053, Run time: 26.65 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0052, Run time: 29.85 seconds\n",
            "\n",
            " Trial 6/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0106, Run time: 32.67 seconds\n",
            "        Epoch [2/8], Loss: 0.0082, Run time: 33.04 seconds\n",
            "        Epoch [3/8], Loss: 0.0073, Run time: 32.26 seconds\n",
            "        Epoch [4/8], Loss: 0.0069, Run time: 34.07 seconds\n",
            "        Epoch [5/8], Loss: 0.0056, Run time: 32.49 seconds\n",
            "        Epoch [6/8], Loss: 0.0057, Run time: 33.46 seconds\n",
            "        Epoch [7/8], Loss: 0.0053, Run time: 31.92 seconds\n",
            "        Epoch [8/8], Loss: 0.0046, Run time: 33.34 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0117, Run time: 26.15 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0090, Run time: 28.62 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0083, Run time: 26.29 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0074, Run time: 28.93 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0063, Run time: 26.35 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0061, Run time: 29.53 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0059, Run time: 26.65 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0055, Run time: 28.94 seconds\n",
            "\n",
            " Trial 7/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0152, Run time: 33.36 seconds\n",
            "        Epoch [2/8], Loss: 0.0095, Run time: 32.25 seconds\n",
            "        Epoch [3/8], Loss: 0.0073, Run time: 33.67 seconds\n",
            "        Epoch [4/8], Loss: 0.0070, Run time: 32.75 seconds\n",
            "        Epoch [5/8], Loss: 0.0064, Run time: 33.80 seconds\n",
            "        Epoch [6/8], Loss: 0.0059, Run time: 32.31 seconds\n",
            "        Epoch [7/8], Loss: 0.0059, Run time: 34.67 seconds\n",
            "        Epoch [8/8], Loss: 0.0052, Run time: 33.54 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0164, Run time: 27.09 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0105, Run time: 28.73 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0083, Run time: 25.92 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0081, Run time: 28.59 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0073, Run time: 26.37 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0064, Run time: 29.02 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0063, Run time: 26.11 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0056, Run time: 29.44 seconds\n",
            "\n",
            " Trial 8/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0154, Run time: 31.97 seconds\n",
            "        Epoch [2/8], Loss: 0.0135, Run time: 32.78 seconds\n",
            "        Epoch [3/8], Loss: 0.0091, Run time: 31.75 seconds\n",
            "        Epoch [4/8], Loss: 0.0076, Run time: 33.10 seconds\n",
            "        Epoch [5/8], Loss: 0.0065, Run time: 32.21 seconds\n",
            "        Epoch [6/8], Loss: 0.0060, Run time: 32.85 seconds\n",
            "        Epoch [7/8], Loss: 0.0059, Run time: 31.66 seconds\n",
            "        Epoch [8/8], Loss: 0.0054, Run time: 32.92 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0186, Run time: 26.31 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0133, Run time: 28.70 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0102, Run time: 26.00 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0089, Run time: 28.50 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0080, Run time: 26.04 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0072, Run time: 29.73 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0068, Run time: 25.95 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0064, Run time: 28.96 seconds\n",
            "\n",
            " Trial 9/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0121, Run time: 32.22 seconds\n",
            "        Epoch [2/8], Loss: 0.0093, Run time: 32.84 seconds\n",
            "        Epoch [3/8], Loss: 0.0079, Run time: 32.17 seconds\n",
            "        Epoch [4/8], Loss: 0.0066, Run time: 32.43 seconds\n",
            "        Epoch [5/8], Loss: 0.0061, Run time: 32.61 seconds\n",
            "        Epoch [6/8], Loss: 0.0061, Run time: 32.68 seconds\n",
            "        Epoch [7/8], Loss: 0.0057, Run time: 32.70 seconds\n",
            "        Epoch [8/8], Loss: 0.0050, Run time: 31.92 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0161, Run time: 26.46 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0103, Run time: 29.07 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0090, Run time: 26.32 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0076, Run time: 29.57 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0072, Run time: 26.14 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0070, Run time: 29.01 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0063, Run time: 26.63 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0054, Run time: 29.02 seconds\n",
            "\n",
            " Trial 10/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0151, Run time: 33.33 seconds\n",
            "        Epoch [2/8], Loss: 0.0097, Run time: 32.05 seconds\n",
            "        Epoch [3/8], Loss: 0.0078, Run time: 33.18 seconds\n",
            "        Epoch [4/8], Loss: 0.0066, Run time: 31.93 seconds\n",
            "        Epoch [5/8], Loss: 0.0061, Run time: 32.96 seconds\n",
            "        Epoch [6/8], Loss: 0.0054, Run time: 32.05 seconds\n",
            "        Epoch [7/8], Loss: 0.0073, Run time: 32.86 seconds\n",
            "        Epoch [8/8], Loss: 0.0050, Run time: 32.01 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0164, Run time: 26.19 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0122, Run time: 29.26 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0084, Run time: 26.01 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0068, Run time: 28.43 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0067, Run time: 26.15 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0058, Run time: 28.67 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0056, Run time: 26.36 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0054, Run time: 28.63 seconds\n",
            "\n",
            " Trial 11/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0157, Run time: 32.61 seconds\n",
            "        Epoch [2/8], Loss: 0.0117, Run time: 32.11 seconds\n",
            "        Epoch [3/8], Loss: 0.0070, Run time: 32.65 seconds\n",
            "        Epoch [4/8], Loss: 0.0068, Run time: 32.46 seconds\n",
            "        Epoch [5/8], Loss: 0.0065, Run time: 32.64 seconds\n",
            "        Epoch [6/8], Loss: 0.0060, Run time: 32.67 seconds\n",
            "        Epoch [7/8], Loss: 0.0065, Run time: 32.18 seconds\n",
            "        Epoch [8/8], Loss: 0.0050, Run time: 32.93 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0189, Run time: 26.75 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0140, Run time: 28.49 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0105, Run time: 25.82 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0092, Run time: 28.60 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0086, Run time: 26.12 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0075, Run time: 28.39 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0075, Run time: 26.11 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0060, Run time: 29.28 seconds\n",
            "\n",
            " Trial 12/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0146, Run time: 31.99 seconds\n",
            "        Epoch [2/8], Loss: 0.0099, Run time: 32.29 seconds\n",
            "        Epoch [3/8], Loss: 0.0078, Run time: 32.08 seconds\n",
            "        Epoch [4/8], Loss: 0.0067, Run time: 33.30 seconds\n",
            "        Epoch [5/8], Loss: 0.0062, Run time: 32.21 seconds\n",
            "        Epoch [6/8], Loss: 0.0059, Run time: 32.94 seconds\n",
            "        Epoch [7/8], Loss: 0.0054, Run time: 31.95 seconds\n",
            "        Epoch [8/8], Loss: 0.0051, Run time: 33.11 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0170, Run time: 26.06 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0113, Run time: 28.34 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0088, Run time: 26.11 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0074, Run time: 29.06 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0068, Run time: 25.97 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0065, Run time: 28.68 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0059, Run time: 26.09 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0056, Run time: 29.32 seconds\n",
            "\n",
            " Trial 13/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0144, Run time: 31.77 seconds\n",
            "        Epoch [2/8], Loss: 0.0094, Run time: 33.05 seconds\n",
            "        Epoch [3/8], Loss: 0.0091, Run time: 31.89 seconds\n",
            "        Epoch [4/8], Loss: 0.0071, Run time: 33.10 seconds\n",
            "        Epoch [5/8], Loss: 0.0060, Run time: 31.69 seconds\n",
            "        Epoch [6/8], Loss: 0.0057, Run time: 32.57 seconds\n",
            "        Epoch [7/8], Loss: 0.0050, Run time: 31.65 seconds\n",
            "        Epoch [8/8], Loss: 0.0051, Run time: 32.57 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0152, Run time: 26.42 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0103, Run time: 28.87 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0097, Run time: 26.12 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0080, Run time: 28.34 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0068, Run time: 26.26 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0066, Run time: 30.09 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0061, Run time: 25.85 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0057, Run time: 28.42 seconds\n",
            "\n",
            " Trial 14/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0151, Run time: 31.99 seconds\n",
            "        Epoch [2/8], Loss: 0.0100, Run time: 33.15 seconds\n",
            "        Epoch [3/8], Loss: 0.0084, Run time: 32.35 seconds\n",
            "        Epoch [4/8], Loss: 0.0064, Run time: 33.17 seconds\n",
            "        Epoch [5/8], Loss: 0.0062, Run time: 32.70 seconds\n",
            "        Epoch [6/8], Loss: 0.0054, Run time: 32.43 seconds\n",
            "        Epoch [7/8], Loss: 0.0053, Run time: 33.21 seconds\n",
            "        Epoch [8/8], Loss: 0.0052, Run time: 32.56 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0185, Run time: 26.13 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0148, Run time: 28.71 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0129, Run time: 26.28 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0097, Run time: 28.70 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0083, Run time: 25.83 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0070, Run time: 28.52 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0068, Run time: 25.92 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0061, Run time: 28.35 seconds\n",
            "\n",
            " Trial 15/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0155, Run time: 32.66 seconds\n",
            "        Epoch [2/8], Loss: 0.0147, Run time: 32.35 seconds\n",
            "        Epoch [3/8], Loss: 0.0101, Run time: 32.34 seconds\n",
            "        Epoch [4/8], Loss: 0.0081, Run time: 32.85 seconds\n",
            "        Epoch [5/8], Loss: 0.0074, Run time: 31.93 seconds\n",
            "        Epoch [6/8], Loss: 0.0072, Run time: 33.10 seconds\n",
            "        Epoch [7/8], Loss: 0.0068, Run time: 31.98 seconds\n",
            "        Epoch [8/8], Loss: 0.0061, Run time: 33.05 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0187, Run time: 25.89 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0145, Run time: 28.42 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0098, Run time: 25.98 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0080, Run time: 29.34 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0076, Run time: 25.75 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0072, Run time: 28.26 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0070, Run time: 26.48 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0063, Run time: 31.85 seconds\n",
            "\n",
            " Trial 16/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0126, Run time: 32.61 seconds\n",
            "        Epoch [2/8], Loss: 0.0085, Run time: 32.27 seconds\n",
            "        Epoch [3/8], Loss: 0.0083, Run time: 31.84 seconds\n",
            "        Epoch [4/8], Loss: 0.0064, Run time: 32.93 seconds\n",
            "        Epoch [5/8], Loss: 0.0066, Run time: 31.83 seconds\n",
            "        Epoch [6/8], Loss: 0.0058, Run time: 32.49 seconds\n",
            "        Epoch [7/8], Loss: 0.0051, Run time: 31.68 seconds\n",
            "        Epoch [8/8], Loss: 0.0046, Run time: 33.30 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0156, Run time: 26.26 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0116, Run time: 28.59 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0095, Run time: 25.72 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0073, Run time: 29.64 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0076, Run time: 26.21 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0065, Run time: 28.71 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0057, Run time: 25.91 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0051, Run time: 28.28 seconds\n",
            "\n",
            " Trial 17/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0140, Run time: 32.64 seconds\n",
            "        Epoch [2/8], Loss: 0.0089, Run time: 31.90 seconds\n",
            "        Epoch [3/8], Loss: 0.0075, Run time: 31.91 seconds\n",
            "        Epoch [4/8], Loss: 0.0072, Run time: 32.09 seconds\n",
            "        Epoch [5/8], Loss: 0.0060, Run time: 32.07 seconds\n",
            "        Epoch [6/8], Loss: 0.0064, Run time: 32.88 seconds\n",
            "        Epoch [7/8], Loss: 0.0051, Run time: 31.57 seconds\n",
            "        Epoch [8/8], Loss: 0.0052, Run time: 32.98 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0135, Run time: 26.13 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0093, Run time: 28.19 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0080, Run time: 25.90 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0073, Run time: 28.45 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0067, Run time: 25.95 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0058, Run time: 29.13 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0052, Run time: 25.81 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0051, Run time: 28.35 seconds\n",
            "\n",
            " Trial 18/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0118, Run time: 32.43 seconds\n",
            "        Epoch [2/8], Loss: 0.0094, Run time: 31.58 seconds\n",
            "        Epoch [3/8], Loss: 0.0083, Run time: 32.49 seconds\n",
            "        Epoch [4/8], Loss: 0.0071, Run time: 32.23 seconds\n",
            "        Epoch [5/8], Loss: 0.0062, Run time: 33.12 seconds\n",
            "        Epoch [6/8], Loss: 0.0057, Run time: 31.69 seconds\n",
            "        Epoch [7/8], Loss: 0.0053, Run time: 32.57 seconds\n",
            "        Epoch [8/8], Loss: 0.0050, Run time: 31.66 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0142, Run time: 26.00 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0102, Run time: 28.81 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0086, Run time: 26.27 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0072, Run time: 28.29 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0066, Run time: 25.96 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0061, Run time: 28.11 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0059, Run time: 26.03 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0054, Run time: 29.60 seconds\n",
            "\n",
            " Trial 19/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0144, Run time: 32.20 seconds\n",
            "        Epoch [2/8], Loss: 0.0108, Run time: 32.64 seconds\n",
            "        Epoch [3/8], Loss: 0.0081, Run time: 31.72 seconds\n",
            "        Epoch [4/8], Loss: 0.0069, Run time: 32.61 seconds\n",
            "        Epoch [5/8], Loss: 0.0066, Run time: 32.17 seconds\n",
            "        Epoch [6/8], Loss: 0.0059, Run time: 32.46 seconds\n",
            "        Epoch [7/8], Loss: 0.0055, Run time: 33.39 seconds\n",
            "        Epoch [8/8], Loss: 0.0056, Run time: 32.05 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0186, Run time: 25.96 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0126, Run time: 29.65 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0116, Run time: 25.73 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0090, Run time: 28.22 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0082, Run time: 26.03 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0061, Run time: 28.47 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0065, Run time: 25.64 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0065, Run time: 29.07 seconds\n",
            "\n",
            " Trial 20/20\n",
            "\n",
            "     Joint Model Training:\n",
            "        Epoch [1/8], Loss: 0.0145, Run time: 31.46 seconds\n",
            "        Epoch [2/8], Loss: 0.0094, Run time: 32.52 seconds\n",
            "        Epoch [3/8], Loss: 0.0081, Run time: 31.32 seconds\n",
            "        Epoch [4/8], Loss: 0.0071, Run time: 32.43 seconds\n",
            "        Epoch [5/8], Loss: 0.0061, Run time: 31.95 seconds\n",
            "        Epoch [6/8], Loss: 0.0058, Run time: 33.28 seconds\n",
            "        Epoch [7/8], Loss: 0.0052, Run time: 31.46 seconds\n",
            "        Epoch [8/8], Loss: 0.0052, Run time: 32.68 seconds\n",
            "\n",
            "     NAM Model Training:\n",
            "        Epoch [1/8] (decoder), Loss: 0.0131, Run time: 25.71 seconds\n",
            "        Epoch [2/8] (encoder), Loss: 0.0092, Run time: 28.45 seconds\n",
            "        Epoch [3/8] (decoder), Loss: 0.0085, Run time: 26.01 seconds\n",
            "        Epoch [4/8] (encoder), Loss: 0.0076, Run time: 29.36 seconds\n",
            "        Epoch [5/8] (decoder), Loss: 0.0065, Run time: 25.82 seconds\n",
            "        Epoch [6/8] (encoder), Loss: 0.0060, Run time: 28.29 seconds\n",
            "        Epoch [7/8] (decoder), Loss: 0.0055, Run time: 25.97 seconds\n",
            "        Epoch [8/8] (encoder), Loss: 0.0052, Run time: 28.92 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train and calculate averages over trials (each trial is a different initialization point)\n",
        "avg_loss_joint, avg_loss_NAM, param_count_joint, param_count_NAM, avg_time_joint, avg_time_NAM = average_training_trials(seed_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ScVxMtpFHl5t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "e966ab9e-b5ab-4210-db85-91a0d3d459d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Average total weight updates per epoch:\n",
            "    Joint Model: 18894645.0\n",
            "    NAM Model (decoder): 9438345.0\n",
            "    NAM Model (encoder): 9456300.0\n",
            "    NAM Model (decoder and encoder): 9447322.5\n",
            "    The Joint Model updates 2.00 times more weights per epoch compared to the NAM Model\n",
            "\n",
            "\n",
            "Average run time per epoch:\n",
            "    Joint Model: 32.67 seconds\n",
            "    NAM Model (decoder): 26.25 seconds\n",
            "    NAM Model (encoder): 28.98 seconds\n",
            "    NAM Model (decoder and encoder): 27.61 seconds\n",
            "    The NAM Model is 15.49% faster than the Joint Model\n"
          ]
        }
      ],
      "source": [
        "plot_joint_model = True\n",
        "plot_NAM_model = True\n",
        "\n",
        "\n",
        "# Calculate number of decoder and decoder epochs for later average calculations\n",
        "if plot_NAM_model:\n",
        "    num_decoder_epochs = sum([1 if param_count_NAM[epoch+1][1]=='d' else 0 for epoch in range(num_epochs)])\n",
        "    num_encoder_epochs = sum([1 if param_count_NAM[epoch+1][1]=='e' else 0 for epoch in range(num_epochs)])\n",
        "    if num_decoder_epochs > 0:\n",
        "        inv_num_decoder_epochs = 1/num_decoder_epochs\n",
        "    else:\n",
        "        inv_num_decoder_epochs = np.nan\n",
        "    if num_encoder_epochs > 0:\n",
        "        inv_num_encoder_epochs = 1/num_encoder_epochs\n",
        "    else:\n",
        "        inv_num_encoder_epochs = np.nan\n",
        "\n",
        "\n",
        "# Plotting the Loss\n",
        "if plot_joint_model:\n",
        "    for epoch in range(num_epochs):\n",
        "        plt.plot(avg_loss_joint[epoch+1][0], avg_loss_joint[epoch+1][1], color='green', linewidth=0.5)\n",
        "\n",
        "\n",
        "if plot_NAM_model:\n",
        "    for epoch in range(num_epochs):\n",
        "        if avg_loss_NAM[epoch+1][2] == 'd':  # plot decoder iteration segments\n",
        "            plt.plot(avg_loss_NAM[epoch+1][0], avg_loss_NAM[epoch+1][1], color='blue', linewidth = 0.5)\n",
        "        else:  # plot encoder iteration segments\n",
        "            plt.plot(avg_loss_NAM[epoch+1][0], avg_loss_NAM[epoch+1][1], color='red', linewidth = 0.5)\n",
        "\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Average Training Loss Over Initializations')\n",
        "plt.xlim([0, num_epochs*num_iter])\n",
        "plt.legend(['Joint Model', 'NAM (decoder)', 'NAM (encoder)'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Average total number of weight updates\n",
        "print('\\n')\n",
        "print(f'Average total weight updates per epoch:')\n",
        "if plot_joint_model:\n",
        "    avg_joint_updates = (1/num_epochs)*sum(param_count_joint[epoch+1] for epoch in range(num_epochs))\n",
        "    print(f'    Joint Model:', avg_joint_updates)\n",
        "\n",
        "    avg_decoder_updates = inv_num_decoder_epochs*sum(param_count_NAM[epoch+1][0] if param_count_NAM[epoch+1][1]=='d' else 0 for epoch in range(num_epochs))\n",
        "    avg_encoder_updates = inv_num_encoder_epochs*sum(param_count_NAM[epoch+1][0] if param_count_NAM[epoch+1][1]=='e' else 0 for epoch in range(num_epochs))\n",
        "    print(f'    NAM Model (decoder):', avg_decoder_updates)\n",
        "    print(f'    NAM Model (encoder):', avg_encoder_updates)\n",
        "    avg_NAM_updates = np.nanmean([avg_decoder_updates, avg_encoder_updates])\n",
        "    print(f'    NAM Model (decoder and encoder):', avg_NAM_updates)\n",
        "if plot_joint_model and plot_NAM_model:\n",
        "    print(f'    The Joint Model updates {avg_joint_updates/avg_NAM_updates:.2f} times more weights per epoch compared to the NAM Model')\n",
        "\n",
        "\n",
        "# Average run time per epoch for each model\n",
        "print('\\n')\n",
        "print(f'Average run time per epoch:')\n",
        "if plot_joint_model:\n",
        "    avg_joint_time = (1/num_epochs)*sum(avg_time_joint[epoch+1] for epoch in range(num_epochs))\n",
        "    print(f'    Joint Model: {avg_joint_time:.2f} seconds')\n",
        "if NAM_model:\n",
        "    avg_decoder_time = inv_num_decoder_epochs*sum(avg_time_NAM[epoch+1][0] if avg_time_NAM[epoch+1][1]=='d' else 0 for epoch in range(num_epochs))\n",
        "    avg_encoder_time = inv_num_encoder_epochs*sum(avg_time_NAM[epoch+1][0] if avg_time_NAM[epoch+1][1]=='e' else 0 for epoch in range(num_epochs))\n",
        "    print(f'    NAM Model (decoder): {avg_decoder_time:.2f} seconds')\n",
        "    print(f'    NAM Model (encoder): {avg_encoder_time:.2f} seconds')\n",
        "    avg_NAM_time = np.nanmean([avg_decoder_time, avg_encoder_time])\n",
        "    print(f'    NAM Model (decoder and encoder): {avg_NAM_time:.2f} seconds')\n",
        "if plot_joint_model and plot_NAM_model:\n",
        "    print(f'    The NAM Model is {(1-avg_NAM_time/avg_joint_time)*100:.2f}% faster than the Joint Model')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9BKCNzzYN0ZIQaswoGP+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}